import os
os.environ["OPENAI_API_KEY"] = ""
from langchain_openai import OpenAIEmbeddings
import numpy as np
import PyPDF2
import re

# DATA SPLITTING AND CLEANING

# Open the PDF file
pdf_file = open('/Users/juanjose.cano/Documents/IA/chatbotAI/Material_AVA_Prospectiva_2.pdf', 'rb')

# Create a PDF reader object
pdf_reader = PyPDF2.PdfReader(pdf_file)

# Create a list to store the text from the PDF
pdf_text = []

# Loop through each page in the PDF
for page_number in range(len(pdf_reader.pages)):
  # Extract the text from the page
  text = pdf_reader.pages[page_number].extract_text()
  
  # Splitting the essay on '.' and '\n' characters and I need to exclude this 'vs.'
  text = re.split(r'(?<!vs.)(?<=[.])\s+|\n', text)
  
  # Remove the '\n' characters and trim the text
  text = [i.replace('\n', '').strip() for i in text]
  
  # Remove the empty strings
  text = [i for i in text if i != '']
  
  # Add the text to the list
  for i in text:
    pdf_text.append(i)
  # if page_number == 10:
    # break
  
# Close the PDF file
pdf_file.close()

# Let's supose we have a list of text like this ['sentence 1.', 'sentence 2', 'sentence 3.', 'sentence 4.']
# I need to join the sentences that are not ending with r'(?<=[.?!])\s+' with the next sentence in a new list
# The new list should look like this ['sentence 1.', 'sentence 2 sentence 3.', 'sentence 4.']

# Create a new list to store the joined sentences
pdf_text_new = []
text_new_sentence = ''

for i in pdf_text:
  if i[-1] in ['.']:
    if text_new_sentence == '':
      r = text_new_sentence
    else:
      r = text_new_sentence + ' ' + i
      text_new_sentence = ''
    if r != '':
      pdf_text_new.append(r)
  else:
    if text_new_sentence == '':
      text_new_sentence = i
    else:
      text_new_sentence += ' ' + i

# turn this into a list of dictionaries (List[dict]), of which, the sentences will be a key-value. 
# Then we can start to add more data to each sentence.
sentences = [{'sentence': x, 'index': i} for i, x in enumerate(pdf_text_new)]

# Great, now that we have our sentences, I want to combine the sentence before and after 
# so that we reduce noise and capture more of the relationships between sequential sentences.

def combine_sentences(sentences, buffer_size=1):
  # Go through each sentence dict
  for i in range(len(sentences)):
    # Create a string to store the combined sentences
    combined_sentence = ''
    
    # Add the previous sentences to the combined sentence
    for j in range(i - buffer_size, i):
      # check if the index is not negative
      if j >= 0:
        # Add the sentence to the combined sentence
        combined_sentence += sentences[j]['sentence'] + ' '
        
    # Add the current sentence to the combined sentence
    combined_sentence += sentences[i]['sentence']
    
    # Add the next sentences to the combined sentence
    for j in range(i + 1, i + buffer_size + 1):
      # check if the index is not out of range
      if j < len(sentences):
        # Add the sentence to the combined sentence
        combined_sentence += ' ' + sentences[j]['sentence']
        
    # Update the current sentence with the combined sentence
    sentences[i]['combined_sentence'] = combined_sentence
    
  return sentences

# Combine the sentences
sentences = combine_sentences(sentences, buffer_size=1)

# Print the text from the PDF
print(sentences[:3])

# EMBEDDINGS


# Let's get our embeddings
oai_embeddings = OpenAIEmbeddings()
embeddings = oai_embeddings.embed_documents([x['combined_sentence'] for x in sentences])
# embeddings = client.embeddings.create(input = [x['combined_sentence'] for x in sentences], model="text-embedding-3-small").data[0].embedding
# embeddings = np.array(embeddings).reshape(-1, 1)
print(embeddings[:3])

# Add the embeddings to the sentences
for i, sentence in enumerate(sentences):
  sentence['combined_sentence_embedding'] = embeddings[i]
  
# Get the cosine distance between the sequential embedding pairs
# We'll add 'distance_to_next' as another key

from sklearn.metrics.pairwise import cosine_similarity

def get_cosine_distances(sentences):
  distances = []
  for i in range(len(sentences) - 1):
    embedding_current = sentences[i]['combined_sentence_embedding']
    embedding_next = sentences[i + 1]['combined_sentence_embedding']
    
    # Calculate the cosine similarity
    similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]
    
    # Convert the cosine similarity to a distance
    distance = 1 - similarity
    
    # Add the distance to the list
    distances.append(distance)
    
    # Store the distance in the dict
    sentences[i]['distance_to_next'] = distance
    
  # Handle the last sentence
  sentences[-1]['distance_to_next'] = 0
  
  return sentences, distances

# Get the cosine distances
sentences, distances = get_cosine_distances(sentences)

# Print the first 3 sentences
print(sentences[:3])

# Print the 'distance_to_next' for each sentence
for sentence in sentences:
    print(sentence['distance_to_next'])

import matplotlib.pyplot as plt

plt.plot(distances)


## PLOTTING

y_upper_bound = .2
plt.ylim(0, y_upper_bound)
plt.xlim(0, len(distances))

# We need to get the distance threshold that we'll consider an outlier
# We'll use numpy .percentile() for this
breakpoint_percentile_threshold = 88 # The higher the percentile, the fewer chunks you'll get
breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff
plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-');

# Then we'll see how many distances are actually above this one
num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold
plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f"{num_distances_above_theshold + 1} Chunks");

# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text
indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list

# Start of the shading and text
colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']
for i, breakpoint_index in enumerate(indices_above_thresh):
    start_index = 0 if i == 0 else indices_above_thresh[i - 1]
    end_index = breakpoint_index if i < len(indices_above_thresh) - 1 else len(distances)

    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)
    plt.text(x=np.average([start_index, end_index]),
             y=breakpoint_distance_threshold + (y_upper_bound)/ 20,
             s=f"Chunk #{i}", horizontalalignment='center',
             rotation='vertical')

# # Additional step to shade from the last breakpoint to the end of the dataset
if indices_above_thresh:
    last_breakpoint = indices_above_thresh[-1]
    if last_breakpoint < len(distances):
        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)
        plt.text(x=np.average([last_breakpoint, len(distances)]),
                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,
                 s=f"Chunk #{i+1}",
                 rotation='vertical')

plt.title("PG Essay Chunks Based On Embedding Breakpoints")
plt.xlabel("Index of sentences in essay (Sentence Position)")
plt.ylabel("Cosine distance between sequential sentences")
plt.show()


## combine the sentences into chunks

# Initialize the start index
start_index = 0

# Create a list to hold the grouped sentences
chunks = []

# Iterate through the breakpoints to slice the sentences
for index in indices_above_thresh:
    # The end index is the current breakpoint
    end_index = index

    # Slice the sentence_dicts from the current start index to the end index
    group = sentences[start_index:end_index + 1]
    combined_text = ' '.join([d['sentence'] for d in group])
    chunks.append(combined_text)
    
    # Update the start index for the next group
    start_index = index + 1

# The last group, if any sentences remain
if start_index < len(sentences):
    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])
    chunks.append(combined_text)

# grouped_sentences now contains the chunked sentences

# Print the first chunk
print(chunks[0])
